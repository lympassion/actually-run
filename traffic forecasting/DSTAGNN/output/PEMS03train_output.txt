Read configuration file: configurations/PEMS03_dstagnn.conf
CUDA: True cuda:0
folder_dir: dstagnn_h1d0w0_channel1_1.000000e-04
params_path: myexperiments/PEMS03/dstagnn_h1d0w0_channel1_1.000000e-04
load file: ./data/PEMS03/PEMS03_r1_d0_w0_dstagnn
train: torch.Size([15711, 358, 1, 12]) torch.Size([15711, 358, 12])
val: torch.Size([5237, 358, 1, 12]) torch.Size([5237, 358, 12])
test: torch.Size([5237, 358, 1, 12]) torch.Size([5237, 358, 12])
delete the old one and create params directory myexperiments/PEMS03/dstagnn_h1d0w0_channel1_1.000000e-04
param list:
CUDA	 cuda:0
in_channels	 1
nb_block	 4
nb_chev_filter	 32
nb_time_filter	 32
time_strides	 1
batch_size	 32
graph_signal_matrix_filename	 ./data/PEMS03/PEMS03.npz
start_epoch	 0
epochs	 90
DSTAGNN_submodule(
  (BlockList): ModuleList(
    (0): DSTAGNN_block(
      (sigmoid): Sigmoid()
      (tanh): Tanh()
      (relu): ReLU(inplace=True)
      (pre_conv): Conv2d(12, 512, kernel_size=(1, 1), stride=(1, 1))
      (EmbedT): Embedding(
        (pos_embed): Embedding(12, 358)
        (norm): LayerNorm((358,), eps=1e-05, elementwise_affine=True)
      )
      (EmbedS): Embedding(
        (pos_embed): Embedding(358, 512)
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (TAt): MultiHeadAttention(
        (W_Q): Linear(in_features=358, out_features=96, bias=False)
        (W_K): Linear(in_features=358, out_features=96, bias=False)
        (W_V): Linear(in_features=358, out_features=96, bias=False)
        (fc): Linear(in_features=96, out_features=358, bias=False)
      )
      (SAt): SMultiHeadAttention(
        (W_Q): Linear(in_features=512, out_features=96, bias=False)
        (W_K): Linear(in_features=512, out_features=96, bias=False)
      )
      (cheb_conv_SAt): cheb_conv_withSAt(
        (relu): ReLU(inplace=True)
        (Theta): ParameterList(
            (0): Parameter containing: [torch.cuda.FloatTensor of size 1x32 (GPU 0)]
            (1): Parameter containing: [torch.cuda.FloatTensor of size 1x32 (GPU 0)]
            (2): Parameter containing: [torch.cuda.FloatTensor of size 1x32 (GPU 0)]
        )
        (mask): ParameterList(
            (0): Parameter containing: [torch.cuda.FloatTensor of size 358x358 (GPU 0)]
            (1): Parameter containing: [torch.cuda.FloatTensor of size 358x358 (GPU 0)]
            (2): Parameter containing: [torch.cuda.FloatTensor of size 358x358 (GPU 0)]
        )
      )
      (gtu3): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 3), stride=(1, 1))
      )
      (gtu5): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 5), stride=(1, 1))
      )
      (gtu7): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 7), stride=(1, 1))
      )
      (pooling): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)
      (residual_conv): Conv2d(1, 32, kernel_size=(1, 1), stride=(1, 1))
      (dropout): Dropout(p=0.05, inplace=False)
      (fcmy): Sequential(
        (0): Linear(in_features=24, out_features=12, bias=True)
        (1): Dropout(p=0.05, inplace=False)
      )
      (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
    )
    (1): DSTAGNN_block(
      (sigmoid): Sigmoid()
      (tanh): Tanh()
      (relu): ReLU(inplace=True)
      (pre_conv): Conv2d(12, 512, kernel_size=(1, 32), stride=(1, 1))
      (EmbedT): Embedding(
        (pos_embed): Embedding(12, 358)
        (norm): LayerNorm((358,), eps=1e-05, elementwise_affine=True)
      )
      (EmbedS): Embedding(
        (pos_embed): Embedding(358, 512)
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (TAt): MultiHeadAttention(
        (W_Q): Linear(in_features=358, out_features=96, bias=False)
        (W_K): Linear(in_features=358, out_features=96, bias=False)
        (W_V): Linear(in_features=358, out_features=96, bias=False)
        (fc): Linear(in_features=96, out_features=358, bias=False)
      )
      (SAt): SMultiHeadAttention(
        (W_Q): Linear(in_features=512, out_features=96, bias=False)
        (W_K): Linear(in_features=512, out_features=96, bias=False)
      )
      (cheb_conv_SAt): cheb_conv_withSAt(
        (relu): ReLU(inplace=True)
        (Theta): ParameterList(
            (0): Parameter containing: [torch.cuda.FloatTensor of size 32x32 (GPU 0)]
            (1): Parameter containing: [torch.cuda.FloatTensor of size 32x32 (GPU 0)]
            (2): Parameter containing: [torch.cuda.FloatTensor of size 32x32 (GPU 0)]
        )
        (mask): ParameterList(
            (0): Parameter containing: [torch.cuda.FloatTensor of size 358x358 (GPU 0)]
            (1): Parameter containing: [torch.cuda.FloatTensor of size 358x358 (GPU 0)]
            (2): Parameter containing: [torch.cuda.FloatTensor of size 358x358 (GPU 0)]
        )
      )
      (gtu3): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 3), stride=(1, 1))
      )
      (gtu5): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 5), stride=(1, 1))
      )
      (gtu7): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 7), stride=(1, 1))
      )
      (pooling): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)
      (residual_conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
      (dropout): Dropout(p=0.05, inplace=False)
      (fcmy): Sequential(
        (0): Linear(in_features=24, out_features=12, bias=True)
        (1): Dropout(p=0.05, inplace=False)
      )
      (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
    )
    (2): DSTAGNN_block(
      (sigmoid): Sigmoid()
      (tanh): Tanh()
      (relu): ReLU(inplace=True)
      (pre_conv): Conv2d(12, 512, kernel_size=(1, 32), stride=(1, 1))
      (EmbedT): Embedding(
        (pos_embed): Embedding(12, 358)
        (norm): LayerNorm((358,), eps=1e-05, elementwise_affine=True)
      )
      (EmbedS): Embedding(
        (pos_embed): Embedding(358, 512)
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (TAt): MultiHeadAttention(
        (W_Q): Linear(in_features=358, out_features=96, bias=False)
        (W_K): Linear(in_features=358, out_features=96, bias=False)
        (W_V): Linear(in_features=358, out_features=96, bias=False)
        (fc): Linear(in_features=96, out_features=358, bias=False)
      )
      (SAt): SMultiHeadAttention(
        (W_Q): Linear(in_features=512, out_features=96, bias=False)
        (W_K): Linear(in_features=512, out_features=96, bias=False)
      )
      (cheb_conv_SAt): cheb_conv_withSAt(
        (relu): ReLU(inplace=True)
        (Theta): ParameterList(
            (0): Parameter containing: [torch.cuda.FloatTensor of size 32x32 (GPU 0)]
            (1): Parameter containing: [torch.cuda.FloatTensor of size 32x32 (GPU 0)]
            (2): Parameter containing: [torch.cuda.FloatTensor of size 32x32 (GPU 0)]
        )
        (mask): ParameterList(
            (0): Parameter containing: [torch.cuda.FloatTensor of size 358x358 (GPU 0)]
            (1): Parameter containing: [torch.cuda.FloatTensor of size 358x358 (GPU 0)]
            (2): Parameter containing: [torch.cuda.FloatTensor of size 358x358 (GPU 0)]
        )
      )
      (gtu3): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 3), stride=(1, 1))
      )
      (gtu5): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 5), stride=(1, 1))
      )
      (gtu7): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 7), stride=(1, 1))
      )
      (pooling): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)
      (residual_conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
      (dropout): Dropout(p=0.05, inplace=False)
      (fcmy): Sequential(
        (0): Linear(in_features=24, out_features=12, bias=True)
        (1): Dropout(p=0.05, inplace=False)
      )
      (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
    )
    (3): DSTAGNN_block(
      (sigmoid): Sigmoid()
      (tanh): Tanh()
      (relu): ReLU(inplace=True)
      (pre_conv): Conv2d(12, 512, kernel_size=(1, 32), stride=(1, 1))
      (EmbedT): Embedding(
        (pos_embed): Embedding(12, 358)
        (norm): LayerNorm((358,), eps=1e-05, elementwise_affine=True)
      )
      (EmbedS): Embedding(
        (pos_embed): Embedding(358, 512)
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (TAt): MultiHeadAttention(
        (W_Q): Linear(in_features=358, out_features=96, bias=False)
        (W_K): Linear(in_features=358, out_features=96, bias=False)
        (W_V): Linear(in_features=358, out_features=96, bias=False)
        (fc): Linear(in_features=96, out_features=358, bias=False)
      )
      (SAt): SMultiHeadAttention(
        (W_Q): Linear(in_features=512, out_features=96, bias=False)
        (W_K): Linear(in_features=512, out_features=96, bias=False)
      )
      (cheb_conv_SAt): cheb_conv_withSAt(
        (relu): ReLU(inplace=True)
        (Theta): ParameterList(
            (0): Parameter containing: [torch.cuda.FloatTensor of size 32x32 (GPU 0)]
            (1): Parameter containing: [torch.cuda.FloatTensor of size 32x32 (GPU 0)]
            (2): Parameter containing: [torch.cuda.FloatTensor of size 32x32 (GPU 0)]
        )
        (mask): ParameterList(
            (0): Parameter containing: [torch.cuda.FloatTensor of size 358x358 (GPU 0)]
            (1): Parameter containing: [torch.cuda.FloatTensor of size 358x358 (GPU 0)]
            (2): Parameter containing: [torch.cuda.FloatTensor of size 358x358 (GPU 0)]
        )
      )
      (gtu3): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 3), stride=(1, 1))
      )
      (gtu5): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 5), stride=(1, 1))
      )
      (gtu7): GTU(
        (tanh): Tanh()
        (sigmoid): Sigmoid()
        (con2out): Conv2d(32, 64, kernel_size=(1, 7), stride=(1, 1))
      )
      (pooling): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)
      (residual_conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
      (dropout): Dropout(p=0.05, inplace=False)
      (fcmy): Sequential(
        (0): Linear(in_features=24, out_features=12, bias=True)
        (1): Dropout(p=0.05, inplace=False)
      )
      (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
    )
  )
  (final_conv): Conv2d(48, 128, kernel_size=(1, 32), stride=(1, 1))
  (final_fc): Linear(in_features=128, out_features=12, bias=True)
)
Net's state_dict:
BlockList.0.pre_conv.weight 	 torch.Size([512, 12, 1, 1])
BlockList.0.pre_conv.bias 	 torch.Size([512])
BlockList.0.EmbedT.pos_embed.weight 	 torch.Size([12, 358])
BlockList.0.EmbedT.norm.weight 	 torch.Size([358])
BlockList.0.EmbedT.norm.bias 	 torch.Size([358])
BlockList.0.EmbedS.pos_embed.weight 	 torch.Size([358, 512])
BlockList.0.EmbedS.norm.weight 	 torch.Size([512])
BlockList.0.EmbedS.norm.bias 	 torch.Size([512])
BlockList.0.TAt.W_Q.weight 	 torch.Size([96, 358])
BlockList.0.TAt.W_K.weight 	 torch.Size([96, 358])
BlockList.0.TAt.W_V.weight 	 torch.Size([96, 358])
BlockList.0.TAt.fc.weight 	 torch.Size([358, 96])
BlockList.0.SAt.W_Q.weight 	 torch.Size([96, 512])
BlockList.0.SAt.W_K.weight 	 torch.Size([96, 512])
BlockList.0.cheb_conv_SAt.Theta.0 	 torch.Size([1, 32])
BlockList.0.cheb_conv_SAt.Theta.1 	 torch.Size([1, 32])
BlockList.0.cheb_conv_SAt.Theta.2 	 torch.Size([1, 32])
BlockList.0.cheb_conv_SAt.mask.0 	 torch.Size([358, 358])
BlockList.0.cheb_conv_SAt.mask.1 	 torch.Size([358, 358])
BlockList.0.cheb_conv_SAt.mask.2 	 torch.Size([358, 358])
BlockList.0.gtu3.con2out.weight 	 torch.Size([64, 32, 1, 3])
BlockList.0.gtu3.con2out.bias 	 torch.Size([64])
BlockList.0.gtu5.con2out.weight 	 torch.Size([64, 32, 1, 5])
BlockList.0.gtu5.con2out.bias 	 torch.Size([64])
BlockList.0.gtu7.con2out.weight 	 torch.Size([64, 32, 1, 7])
BlockList.0.gtu7.con2out.bias 	 torch.Size([64])
BlockList.0.residual_conv.weight 	 torch.Size([32, 1, 1, 1])
BlockList.0.residual_conv.bias 	 torch.Size([32])
BlockList.0.fcmy.0.weight 	 torch.Size([12, 24])
BlockList.0.fcmy.0.bias 	 torch.Size([12])
BlockList.0.ln.weight 	 torch.Size([32])
BlockList.0.ln.bias 	 torch.Size([32])
BlockList.1.pre_conv.weight 	 torch.Size([512, 12, 1, 32])
BlockList.1.pre_conv.bias 	 torch.Size([512])
BlockList.1.EmbedT.pos_embed.weight 	 torch.Size([12, 358])
BlockList.1.EmbedT.norm.weight 	 torch.Size([358])
BlockList.1.EmbedT.norm.bias 	 torch.Size([358])
BlockList.1.EmbedS.pos_embed.weight 	 torch.Size([358, 512])
BlockList.1.EmbedS.norm.weight 	 torch.Size([512])
BlockList.1.EmbedS.norm.bias 	 torch.Size([512])
BlockList.1.TAt.W_Q.weight 	 torch.Size([96, 358])
BlockList.1.TAt.W_K.weight 	 torch.Size([96, 358])
BlockList.1.TAt.W_V.weight 	 torch.Size([96, 358])
BlockList.1.TAt.fc.weight 	 torch.Size([358, 96])
BlockList.1.SAt.W_Q.weight 	 torch.Size([96, 512])
BlockList.1.SAt.W_K.weight 	 torch.Size([96, 512])
BlockList.1.cheb_conv_SAt.Theta.0 	 torch.Size([32, 32])
BlockList.1.cheb_conv_SAt.Theta.1 	 torch.Size([32, 32])
BlockList.1.cheb_conv_SAt.Theta.2 	 torch.Size([32, 32])
BlockList.1.cheb_conv_SAt.mask.0 	 torch.Size([358, 358])
BlockList.1.cheb_conv_SAt.mask.1 	 torch.Size([358, 358])
BlockList.1.cheb_conv_SAt.mask.2 	 torch.Size([358, 358])
BlockList.1.gtu3.con2out.weight 	 torch.Size([64, 32, 1, 3])
BlockList.1.gtu3.con2out.bias 	 torch.Size([64])
BlockList.1.gtu5.con2out.weight 	 torch.Size([64, 32, 1, 5])
BlockList.1.gtu5.con2out.bias 	 torch.Size([64])
BlockList.1.gtu7.con2out.weight 	 torch.Size([64, 32, 1, 7])
BlockList.1.gtu7.con2out.bias 	 torch.Size([64])
BlockList.1.residual_conv.weight 	 torch.Size([32, 32, 1, 1])
BlockList.1.residual_conv.bias 	 torch.Size([32])
BlockList.1.fcmy.0.weight 	 torch.Size([12, 24])
BlockList.1.fcmy.0.bias 	 torch.Size([12])
BlockList.1.ln.weight 	 torch.Size([32])
BlockList.1.ln.bias 	 torch.Size([32])
BlockList.2.pre_conv.weight 	 torch.Size([512, 12, 1, 32])
BlockList.2.pre_conv.bias 	 torch.Size([512])
BlockList.2.EmbedT.pos_embed.weight 	 torch.Size([12, 358])
BlockList.2.EmbedT.norm.weight 	 torch.Size([358])
BlockList.2.EmbedT.norm.bias 	 torch.Size([358])
BlockList.2.EmbedS.pos_embed.weight 	 torch.Size([358, 512])
BlockList.2.EmbedS.norm.weight 	 torch.Size([512])
BlockList.2.EmbedS.norm.bias 	 torch.Size([512])
BlockList.2.TAt.W_Q.weight 	 torch.Size([96, 358])
BlockList.2.TAt.W_K.weight 	 torch.Size([96, 358])
BlockList.2.TAt.W_V.weight 	 torch.Size([96, 358])
BlockList.2.TAt.fc.weight 	 torch.Size([358, 96])
BlockList.2.SAt.W_Q.weight 	 torch.Size([96, 512])
BlockList.2.SAt.W_K.weight 	 torch.Size([96, 512])
BlockList.2.cheb_conv_SAt.Theta.0 	 torch.Size([32, 32])
BlockList.2.cheb_conv_SAt.Theta.1 	 torch.Size([32, 32])
BlockList.2.cheb_conv_SAt.Theta.2 	 torch.Size([32, 32])
BlockList.2.cheb_conv_SAt.mask.0 	 torch.Size([358, 358])
BlockList.2.cheb_conv_SAt.mask.1 	 torch.Size([358, 358])
BlockList.2.cheb_conv_SAt.mask.2 	 torch.Size([358, 358])
BlockList.2.gtu3.con2out.weight 	 torch.Size([64, 32, 1, 3])
BlockList.2.gtu3.con2out.bias 	 torch.Size([64])
BlockList.2.gtu5.con2out.weight 	 torch.Size([64, 32, 1, 5])
BlockList.2.gtu5.con2out.bias 	 torch.Size([64])
BlockList.2.gtu7.con2out.weight 	 torch.Size([64, 32, 1, 7])
BlockList.2.gtu7.con2out.bias 	 torch.Size([64])
BlockList.2.residual_conv.weight 	 torch.Size([32, 32, 1, 1])
BlockList.2.residual_conv.bias 	 torch.Size([32])
BlockList.2.fcmy.0.weight 	 torch.Size([12, 24])
BlockList.2.fcmy.0.bias 	 torch.Size([12])
BlockList.2.ln.weight 	 torch.Size([32])
BlockList.2.ln.bias 	 torch.Size([32])
BlockList.3.pre_conv.weight 	 torch.Size([512, 12, 1, 32])
BlockList.3.pre_conv.bias 	 torch.Size([512])
BlockList.3.EmbedT.pos_embed.weight 	 torch.Size([12, 358])
BlockList.3.EmbedT.norm.weight 	 torch.Size([358])
BlockList.3.EmbedT.norm.bias 	 torch.Size([358])
BlockList.3.EmbedS.pos_embed.weight 	 torch.Size([358, 512])
BlockList.3.EmbedS.norm.weight 	 torch.Size([512])
BlockList.3.EmbedS.norm.bias 	 torch.Size([512])
BlockList.3.TAt.W_Q.weight 	 torch.Size([96, 358])
BlockList.3.TAt.W_K.weight 	 torch.Size([96, 358])
BlockList.3.TAt.W_V.weight 	 torch.Size([96, 358])
BlockList.3.TAt.fc.weight 	 torch.Size([358, 96])
BlockList.3.SAt.W_Q.weight 	 torch.Size([96, 512])
BlockList.3.SAt.W_K.weight 	 torch.Size([96, 512])
BlockList.3.cheb_conv_SAt.Theta.0 	 torch.Size([32, 32])
BlockList.3.cheb_conv_SAt.Theta.1 	 torch.Size([32, 32])
BlockList.3.cheb_conv_SAt.Theta.2 	 torch.Size([32, 32])
BlockList.3.cheb_conv_SAt.mask.0 	 torch.Size([358, 358])
BlockList.3.cheb_conv_SAt.mask.1 	 torch.Size([358, 358])
BlockList.3.cheb_conv_SAt.mask.2 	 torch.Size([358, 358])
BlockList.3.gtu3.con2out.weight 	 torch.Size([64, 32, 1, 3])
BlockList.3.gtu3.con2out.bias 	 torch.Size([64])
BlockList.3.gtu5.con2out.weight 	 torch.Size([64, 32, 1, 5])
BlockList.3.gtu5.con2out.bias 	 torch.Size([64])
BlockList.3.gtu7.con2out.weight 	 torch.Size([64, 32, 1, 7])
BlockList.3.gtu7.con2out.bias 	 torch.Size([64])
BlockList.3.residual_conv.weight 	 torch.Size([32, 32, 1, 1])
BlockList.3.residual_conv.bias 	 torch.Size([32])
BlockList.3.fcmy.0.weight 	 torch.Size([12, 24])
BlockList.3.fcmy.0.bias 	 torch.Size([12])
BlockList.3.ln.weight 	 torch.Size([32])
BlockList.3.ln.bias 	 torch.Size([32])
final_conv.weight 	 torch.Size([128, 48, 1, 32])
final_conv.bias 	 torch.Size([128])
final_fc.weight 	 torch.Size([12, 128])
final_fc.bias 	 torch.Size([12])
Net's total params: 4172348
Optimizer's state_dict:
state 	 {}
param_groups 	 [{'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131]}]
current epoch:  0
validation batch 1 / 164, loss: 292.56
validation batch 101 / 164, loss: 218.25
val loss 178.8322062376069
best epoch:  0
best val loss:  178.8322062376069
save parameters to file: myexperiments/PEMS03/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_0.params
current epoch:  1
validation batch 1 / 164, loss: 34.59
validation batch 101 / 164, loss: 31.51
val loss 21.781525294955184
best epoch:  1
best val loss:  21.781525294955184
save parameters to file: myexperiments/PEMS03/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_1.params
current epoch:  2
validation batch 1 / 164, loss: 24.99
validation batch 101 / 164, loss: 26.31
val loss 18.053107878056966
best epoch:  2
best val loss:  18.053107878056966
save parameters to file: myexperiments/PEMS03/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_2.params
global step: 1000, training loss: 18.72, time: 573.14s
current epoch:  3
validation batch 1 / 164, loss: 22.95
validation batch 101 / 164, loss: 25.47
val loss 16.841252219386217
best epoch:  3
best val loss:  16.841252219386217
save parameters to file: myexperiments/PEMS03/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_3.params
current epoch:  4
validation batch 1 / 164, loss: 21.51
validation batch 101 / 164, loss: 22.95
val loss 16.10685998928256
best epoch:  4
best val loss:  16.10685998928256
save parameters to file: myexperiments/PEMS03/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_4.params
global step: 2000, training loss: 15.29, time: 1123.00s
current epoch:  5
validation batch 1 / 164, loss: 20.79
validation batch 101 / 164, loss: 22.42
val loss 15.662704403807478
best epoch:  5
best val loss:  15.662704403807478
save parameters to file: myexperiments/PEMS03/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_5.params
current epoch:  6
validation batch 1 / 164, loss: 20.51
validation batch 101 / 164, loss: 22.86
val loss 15.538499718759118
best epoch:  6
best val loss:  15.538499718759118
save parameters to file: myexperiments/PEMS03/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_6.params
global step: 3000, training loss: 15.65, time: 1672.72s
current epoch:  7
validation batch 1 / 164, loss: 20.01
validation batch 101 / 164, loss: 22.73
val loss 15.32242214679718
best epoch:  7
best val loss:  15.32242214679718
save parameters to file: myexperiments/PEMS03/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_7.params
current epoch:  8
validation batch 1 / 164, loss: 19.72
validation batch 101 / 164, loss: 22.29
val loss 15.085772598662027
best epoch:  8
best val loss:  15.085772598662027
save parameters to file: myexperiments/PEMS03/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_8.params
global step: 4000, training loss: 14.02, time: 2222.65s
current epoch:  9
validation batch 1 / 164, loss: 19.53
validation batch 101 / 164, loss: 22.37
val loss 15.038770393627447
best epoch:  9
best val loss:  15.038770393627447
save parameters to file: myexperiments/PEMS03/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_9.params
current epoch:  10
validation batch 1 / 164, loss: 19.49
validation batch 101 / 164, loss: 21.68
val loss 14.845458292379611
best epoch:  10
best val loss:  14.845458292379611
save parameters to file: myexperiments/PEMS03/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_10.params
global step: 5000, training loss: 13.66, time: 2777.44s
current epoch:  11
validation batch 1 / 164, loss: 19.45
validation batch 101 / 164, loss: 22.23
val loss 14.775976913731272
best epoch:  11
best val loss:  14.775976913731272
save parameters to file: myexperiments/PEMS03/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_11.params
current epoch:  12
validation batch 1 / 164, loss: 19.15
validation batch 101 / 164, loss: 20.70
val loss 14.67596337562654
best epoch:  12
best val loss:  14.67596337562654
save parameters to file: myexperiments/PEMS03/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_12.params
global step: 6000, training loss: 13.66, time: 3340.86s
current epoch:  13
validation batch 1 / 164, loss: 19.19
validation batch 101 / 164, loss: 22.23
val loss 14.690716118347353
current epoch:  14
validation batch 1 / 164, loss: 19.14
validation batch 101 / 164, loss: 21.70
val loss 14.62620518265701
best epoch:  14
best val loss:  14.62620518265701
save parameters to file: myexperiments/PEMS03/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_14.params
global step: 7000, training loss: 15.24, time: 3908.91s
current epoch:  15
validation batch 1 / 164, loss: 19.13
validation batch 101 / 164, loss: 20.51
val loss 14.72198660199235
current epoch:  16
validation batch 1 / 164, loss: 19.07
validation batch 101 / 164, loss: 21.06
val loss 14.469720226962393
best epoch:  16
best val loss:  14.469720226962393
save parameters to file: myexperiments/PEMS03/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_16.params
global step: 8000, training loss: 14.53, time: 4476.83s
current epoch:  17
validation batch 1 / 164, loss: 18.93
validation batch 101 / 164, loss: 20.38
val loss 14.446197777259641
best epoch:  17
best val loss:  14.446197777259641
save parameters to file: myexperiments/PEMS03/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_17.params
current epoch:  18
validation batch 1 / 164, loss: 18.96
validation batch 101 / 164, loss: 19.82
val loss 14.437617575250021
best epoch:  18
best val loss:  14.437617575250021
save parameters to file: myexperiments/PEMS03/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_18.params
global step: 9000, training loss: 13.43, time: 5045.55s
current epoch:  19
validation batch 1 / 164, loss: 19.06
validation batch 101 / 164, loss: 21.10
val loss 14.399160902674605
best epoch:  19
best val loss:  14.399160902674605
save parameters to file: myexperiments/PEMS03/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_19.params
current epoch:  20
validation batch 1 / 164, loss: 18.84
validation batch 101 / 164, loss: 20.54
val loss 14.34166548601011
best epoch:  20
best val loss:  14.34166548601011
save parameters to file: myexperiments/PEMS03/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_20.params
global step: 10000, training loss: 13.27, time: 5614.68s
current epoch:  21
validation batch 1 / 164, loss: 18.95
validation batch 101 / 164, loss: 21.76
val loss 14.485382862207366
current epoch:  22
validation batch 1 / 164, loss: 18.82
validation batch 101 / 164, loss: 19.81
val loss 14.364469548551048
global step: 11000, training loss: 12.59, time: 6183.17s
current epoch:  23
validation batch 1 / 164, loss: 18.72
validation batch 101 / 164, loss: 22.05
val loss 14.380074643507236
current epoch:  24
validation batch 1 / 164, loss: 18.67
validation batch 101 / 164, loss: 20.26
val loss 14.237981371763276
best epoch:  24
best val loss:  14.237981371763276
save parameters to file: myexperiments/PEMS03/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_24.params
global step: 12000, training loss: 14.13, time: 6752.05s
current epoch:  25
validation batch 1 / 164, loss: 18.76
validation batch 101 / 164, loss: 21.58
val loss 14.291749137203867
current epoch:  26
validation batch 1 / 164, loss: 18.70
validation batch 101 / 164, loss: 20.54
val loss 14.228626047692648
best epoch:  26
best val loss:  14.228626047692648
save parameters to file: myexperiments/PEMS03/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_26.params
global step: 13000, training loss: 15.00, time: 7322.36s
current epoch:  27
validation batch 1 / 164, loss: 18.63
validation batch 101 / 164, loss: 20.31
val loss 14.167385368812376
best epoch:  27
best val loss:  14.167385368812376
save parameters to file: myexperiments/PEMS03/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_27.params
current epoch:  28
validation batch 1 / 164, loss: 18.62
validation batch 101 / 164, loss: 19.74
val loss 14.167190862864983
best epoch:  28
best val loss:  14.167190862864983
save parameters to file: myexperiments/PEMS03/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_28.params
global step: 14000, training loss: 11.31, time: 7892.33s
current epoch:  29
validation batch 1 / 164, loss: 18.57
validation batch 101 / 164, loss: 19.96
val loss 14.163166150814149
best epoch:  29
best val loss:  14.163166150814149
save parameters to file: myexperiments/PEMS03/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_29.params
current epoch:  30
validation batch 1 / 164, loss: 18.52
validation batch 101 / 164, loss: 21.40
val loss 14.178346939203216
global step: 15000, training loss: 14.12, time: 8464.02s
current epoch:  31
validation batch 1 / 164, loss: 18.60
validation batch 101 / 164, loss: 21.36
val loss 14.204662256124543
current epoch:  32
validation batch 1 / 164, loss: 18.71
validation batch 101 / 164, loss: 20.94
val loss 14.246166787496428
global step: 16000, training loss: 14.21, time: 9034.93s
current epoch:  33
validation batch 1 / 164, loss: 18.61
validation batch 101 / 164, loss: 20.57
val loss 14.135260398794966
best epoch:  33
best val loss:  14.135260398794966
save parameters to file: myexperiments/PEMS03/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_33.params
current epoch:  34
validation batch 1 / 164, loss: 18.45
validation batch 101 / 164, loss: 20.44
val loss 14.120646814020668
best epoch:  34
best val loss:  14.120646814020668
save parameters to file: myexperiments/PEMS03/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_34.params
global step: 17000, training loss: 13.53, time: 9604.86s
current epoch:  35
validation batch 1 / 164, loss: 18.42
validation batch 101 / 164, loss: 19.87
val loss 14.069224238395691
best epoch:  35
best val loss:  14.069224238395691
save parameters to file: myexperiments/PEMS03/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_35.params
current epoch:  36
validation batch 1 / 164, loss: 18.51
validation batch 101 / 164, loss: 19.91
val loss 14.071517121501085
global step: 18000, training loss: 13.57, time: 10167.71s
current epoch:  37
validation batch 1 / 164, loss: 18.44
validation batch 101 / 164, loss: 20.22
val loss 14.080392037949911
current epoch:  38
validation batch 1 / 164, loss: 18.48
validation batch 101 / 164, loss: 19.62
val loss 14.067845844640964
best epoch:  38
best val loss:  14.067845844640964
save parameters to file: myexperiments/PEMS03/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_38.params
global step: 19000, training loss: 11.98, time: 10719.84s
current epoch:  39
validation batch 1 / 164, loss: 18.35
validation batch 101 / 164, loss: 20.58
val loss 14.115996104914968
current epoch:  40
validation batch 1 / 164, loss: 18.34
validation batch 101 / 164, loss: 19.37
val loss 14.062290767344033
best epoch:  40
best val loss:  14.062290767344033
save parameters to file: myexperiments/PEMS03/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_40.params
global step: 20000, training loss: 13.05, time: 11271.72s
current epoch:  41
validation batch 1 / 164, loss: 18.42
validation batch 101 / 164, loss: 20.95
val loss 14.118447606156511
current epoch:  42
validation batch 1 / 164, loss: 18.34
validation batch 101 / 164, loss: 19.92
val loss 14.023926112709976
best epoch:  42
best val loss:  14.023926112709976
save parameters to file: myexperiments/PEMS03/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_42.params
global step: 21000, training loss: 11.74, time: 11823.85s
current epoch:  43
validation batch 1 / 164, loss: 18.37
validation batch 101 / 164, loss: 20.01
val loss 14.012387246620364
best epoch:  43
best val loss:  14.012387246620364
save parameters to file: myexperiments/PEMS03/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_43.params
current epoch:  44
validation batch 1 / 164, loss: 18.33
validation batch 101 / 164, loss: 19.91
val loss 14.057602914368234
global step: 22000, training loss: 14.59, time: 12375.88s
current epoch:  45
validation batch 1 / 164, loss: 18.35
validation batch 101 / 164, loss: 21.17
val loss 14.158658696383965
current epoch:  46
validation batch 1 / 164, loss: 18.56
validation batch 101 / 164, loss: 21.81
val loss 14.355755337854712
global step: 23000, training loss: 13.79, time: 12927.90s
current epoch:  47
validation batch 1 / 164, loss: 18.50
validation batch 101 / 164, loss: 20.45
val loss 14.076619421563498
current epoch:  48
validation batch 1 / 164, loss: 18.37
validation batch 101 / 164, loss: 20.05
val loss 14.00659048848036
best epoch:  48
best val loss:  14.00659048848036
save parameters to file: myexperiments/PEMS03/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_48.params
global step: 24000, training loss: 13.12, time: 13480.19s
current epoch:  49
validation batch 1 / 164, loss: 18.39
validation batch 101 / 164, loss: 20.44
val loss 14.008023023605347
current epoch:  50
validation batch 1 / 164, loss: 18.34
validation batch 101 / 164, loss: 19.99
val loss 13.9736876226053
best epoch:  50
best val loss:  13.9736876226053
save parameters to file: myexperiments/PEMS03/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_50.params
global step: 25000, training loss: 12.44, time: 14032.32s
current epoch:  51
validation batch 1 / 164, loss: 18.27
validation batch 101 / 164, loss: 20.91
val loss 14.02367728803216
current epoch:  52
validation batch 1 / 164, loss: 18.32
validation batch 101 / 164, loss: 19.94
val loss 14.04025285709195
global step: 26000, training loss: 13.76, time: 14584.53s
current epoch:  53
validation batch 1 / 164, loss: 18.27
validation batch 101 / 164, loss: 19.55
val loss 13.969728940870704
best epoch:  53
best val loss:  13.969728940870704
save parameters to file: myexperiments/PEMS03/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_53.params
current epoch:  54
validation batch 1 / 164, loss: 18.40
validation batch 101 / 164, loss: 20.09
val loss 13.998598962295347
global step: 27000, training loss: 12.92, time: 15138.85s
current epoch:  55
validation batch 1 / 164, loss: 18.39
validation batch 101 / 164, loss: 20.34
val loss 13.971394253940117
current epoch:  56
validation batch 1 / 164, loss: 18.42
validation batch 101 / 164, loss: 20.42
val loss 13.986067713760749
current epoch:  57
validation batch 1 / 164, loss: 18.34
validation batch 101 / 164, loss: 20.58
val loss 14.024589128610565
global step: 28000, training loss: 12.21, time: 15731.58s
current epoch:  58
validation batch 1 / 164, loss: 18.28
validation batch 101 / 164, loss: 18.96
val loss 13.993066619082194
current epoch:  59
validation batch 1 / 164, loss: 18.28
validation batch 101 / 164, loss: 20.22
val loss 14.013720550188204
global step: 29000, training loss: 13.31, time: 16306.02s
current epoch:  60
validation batch 1 / 164, loss: 18.31
validation batch 101 / 164, loss: 20.15
val loss 13.99421366540397
current epoch:  61
validation batch 1 / 164, loss: 18.32
validation batch 101 / 164, loss: 19.54
val loss 13.92725502863163
best epoch:  61
best val loss:  13.92725502863163
save parameters to file: myexperiments/PEMS03/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_61.params
global step: 30000, training loss: 13.15, time: 16884.42s
current epoch:  62
validation batch 1 / 164, loss: 18.37
validation batch 101 / 164, loss: 19.94
val loss 13.940820028142232
current epoch:  63
validation batch 1 / 164, loss: 18.36
validation batch 101 / 164, loss: 19.79
val loss 13.935466519216211
global step: 31000, training loss: 12.55, time: 17476.46s
current epoch:  64
validation batch 1 / 164, loss: 18.35
validation batch 101 / 164, loss: 19.68
val loss 13.959055720306024
current epoch:  65
validation batch 1 / 164, loss: 18.31
validation batch 101 / 164, loss: 20.21
val loss 13.97286798023596
global step: 32000, training loss: 12.71, time: 18077.84s
current epoch:  66
validation batch 1 / 164, loss: 18.46
validation batch 101 / 164, loss: 20.22
val loss 14.026547051057584
current epoch:  67
validation batch 1 / 164, loss: 18.40
validation batch 101 / 164, loss: 19.64
val loss 13.968526092971244
global step: 33000, training loss: 11.26, time: 18683.07s
current epoch:  68
validation batch 1 / 164, loss: 18.47
validation batch 101 / 164, loss: 19.96
val loss 13.954736244387743
current epoch:  69
validation batch 1 / 164, loss: 18.50
validation batch 101 / 164, loss: 20.39
val loss 14.033494708014697
global step: 34000, training loss: 12.17, time: 19286.97s
current epoch:  70
validation batch 1 / 164, loss: 18.32
validation batch 101 / 164, loss: 19.59
val loss 13.884086254166395
best epoch:  70
best val loss:  13.884086254166395
save parameters to file: myexperiments/PEMS03/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_70.params
current epoch:  71
validation batch 1 / 164, loss: 18.41
validation batch 101 / 164, loss: 19.49
val loss 13.91105977209603
global step: 35000, training loss: 11.48, time: 19890.10s
current epoch:  72
validation batch 1 / 164, loss: 18.41
validation batch 101 / 164, loss: 20.49
val loss 14.010037320416147
current epoch:  73
validation batch 1 / 164, loss: 18.40
validation batch 101 / 164, loss: 19.40
val loss 13.878230301345267
best epoch:  73
best val loss:  13.878230301345267
save parameters to file: myexperiments/PEMS03/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_73.params
global step: 36000, training loss: 12.87, time: 20493.85s
current epoch:  74
validation batch 1 / 164, loss: 18.29
validation batch 101 / 164, loss: 19.85
val loss 13.969908455523049
current epoch:  75
validation batch 1 / 164, loss: 18.35
validation batch 101 / 164, loss: 19.72
val loss 13.939114867187127
global step: 37000, training loss: 12.58, time: 21096.15s
current epoch:  76
validation batch 1 / 164, loss: 18.29
validation batch 101 / 164, loss: 19.99
val loss 13.954043420349679
current epoch:  77
validation batch 1 / 164, loss: 18.40
validation batch 101 / 164, loss: 19.40
val loss 13.948650758440902
global step: 38000, training loss: 11.71, time: 21700.44s
current epoch:  78
validation batch 1 / 164, loss: 18.24
validation batch 101 / 164, loss: 20.12
val loss 13.97908641652363
current epoch:  79
validation batch 1 / 164, loss: 18.34
validation batch 101 / 164, loss: 19.49
val loss 13.922979192035955
global step: 39000, training loss: 11.49, time: 22305.81s
current epoch:  80
validation batch 1 / 164, loss: 18.45
validation batch 101 / 164, loss: 19.90
val loss 13.9398565932018
current epoch:  81
validation batch 1 / 164, loss: 18.34
validation batch 101 / 164, loss: 19.80
val loss 13.961475081560089
global step: 40000, training loss: 11.74, time: 22911.37s
current epoch:  82
validation batch 1 / 164, loss: 18.28
validation batch 101 / 164, loss: 20.51
val loss 13.962715640300658
current epoch:  83
validation batch 1 / 164, loss: 18.38
validation batch 101 / 164, loss: 19.94
val loss 13.940167034544595
global step: 41000, training loss: 11.84, time: 23515.37s
current epoch:  84
validation batch 1 / 164, loss: 18.40
validation batch 101 / 164, loss: 19.92
val loss 13.932392370410081
current epoch:  85
validation batch 1 / 164, loss: 18.36
validation batch 101 / 164, loss: 19.80
val loss 13.97074268503887
global step: 42000, training loss: 12.94, time: 24116.44s
current epoch:  86
validation batch 1 / 164, loss: 18.49
validation batch 101 / 164, loss: 19.67
val loss 13.965146724770708
current epoch:  87
validation batch 1 / 164, loss: 18.44
validation batch 101 / 164, loss: 19.36
val loss 13.902825370067504
global step: 43000, training loss: 12.92, time: 24719.22s
current epoch:  88
validation batch 1 / 164, loss: 18.44
validation batch 101 / 164, loss: 20.19
val loss 14.02857090205681
current epoch:  89
validation batch 1 / 164, loss: 18.34
validation batch 101 / 164, loss: 19.33
val loss 13.930637955665588
global step: 44000, training loss: 12.68, time: 25323.38s
best epoch: 73
load weight from: myexperiments/PEMS03/dstagnn_h1d0w0_channel1_1.000000e-04/epoch_73.params
predicting data set batch 1 / 164
predicting data set batch 101 / 164
input: (5237, 358, 1, 12)
prediction: (5237, 358, 12)
data_target_tensor: (5237, 358, 12)
current epoch: 73, predict 1-th point
MAE: 12.91
RMSE: 22.41
MAPE: 12.66
current epoch: 73, predict 2-th point
MAE: 13.65
RMSE: 23.75
MAPE: 13.28
current epoch: 73, predict 3-th point
MAE: 14.32
RMSE: 24.94
MAPE: 13.81
current epoch: 73, predict 4-th point
MAE: 14.93
RMSE: 26.02
MAPE: 14.21
current epoch: 73, predict 5-th point
MAE: 15.45
RMSE: 26.94
MAPE: 14.58
current epoch: 73, predict 6-th point
MAE: 15.90
RMSE: 27.71
MAPE: 14.98
current epoch: 73, predict 7-th point
MAE: 16.28
RMSE: 28.32
MAPE: 15.36
current epoch: 73, predict 8-th point
MAE: 16.54
RMSE: 28.73
MAPE: 15.49
current epoch: 73, predict 9-th point
MAE: 16.73
RMSE: 29.00
MAPE: 15.59
current epoch: 73, predict 10-th point
MAE: 16.90
RMSE: 29.25
MAPE: 15.75
current epoch: 73, predict 11-th point
MAE: 17.16
RMSE: 29.62
MAPE: 16.00
current epoch: 73, predict 12-th point
MAE: 17.72
RMSE: 30.37
MAPE: 16.48
all MAE: 15.71
all RMSE: 27.36
all MAPE: 14.85
[12.91176, 22.411548280694277, 12.658815085887909, 13.647521, 23.746734715913444, 13.28035444021225, 14.319508, 24.94374774884104, 13.81184458732605, 14.926035, 26.021258539818817, 14.20876681804657, 15.447462, 26.942047089395185, 14.583124220371246, 15.90077, 27.7072975213661, 14.980702102184296, 16.278292, 28.315929617746747, 15.357425808906555, 16.543299, 28.728638251085254, 15.48842042684555, 16.725534, 28.998621414565772, 15.587876737117767, 16.901266, 29.24960352963139, 15.745371580123901, 17.161993, 29.615656284606775, 15.998904407024384, 17.717133, 30.36774624839702, 16.479137539863586, 15.706726, 27.35861091701466, 14.848385751247406]
